Examples
========

This page contains practical examples demonstrating the capabilities of the CE-QUAL-W2 Python Library.

.. note::
   For comprehensive usage examples with detailed explanations, see the 
   `EXAMPLES.md <https://github.com/ecohydrology/cequalw2-Claude/blob/main/docs/EXAMPLES.md>`_ 
   file in the documentation directory.

Quick Start Examples
--------------------

Basic File Reading
~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.fileio import read
    from cequalw2.utils import day_of_year_to_date
    
    # Read a CE-QUAL-W2 output file
    data = read('temperature_output.csv', year=2023, data_columns=['temp', 'do'])
    
    print(f"Data shape: {data.shape}")
    print(f"Date range: {data.index.min()} to {data.index.max()}")
    print(data.head())

Simple Visualization
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.visualization import plot
    
    # Create a basic plot
    fig = plot(data, ylabel='Water Quality Parameters', title='Model Results')
    fig.savefig('water_quality.png', dpi=300, bbox_inches='tight')
    fig.show()

Date Conversion
~~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.utils import day_of_year_to_date
    
    # Convert CE-QUAL-W2 day-of-year to datetime
    day_values = [1.0, 91.5, 182.0, 273.5, 365.0]
    dates = day_of_year_to_date(2023, day_values)
    
    for day, date in zip(day_values, dates):
        print(f"Day {day:6.1f} -> {date.strftime('%Y-%m-%d %H:%M')}")

File Format Examples
--------------------

Reading Different Formats
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.fileio import read, read_csv, read_npt_opt, read_excel, read_sqlite
    
    # CSV files (automatic detection)
    csv_data = read('output.csv', year=2023, data_columns=['temp', 'do'])
    
    # Fixed-width NPT/OPT files
    npt_data = read('output.npt', year=2023, data_columns=['temp', 'do'])
    
    # Excel files
    excel_data = read_excel('model_results.xlsx', sheet_name='Data')
    
    # SQLite databases
    db_data = read_sqlite('results.db', table_name='water_quality')

Writing Data
~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.analysis import write_csv
    from cequalw2.fileio import write_hdf
    
    # Export to CE-QUAL-W2 CSV format
    write_csv(data, 'exported_data.csv', year=2023, 
              header='Model Export\nGenerated by Python Library\n')
    
    # Export to HDF5 for efficient storage
    write_hdf(data, group='model_results', outfile='data.h5')

Visualization Examples
----------------------

Multiple Plot Types
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.visualization import plot, multi_plot, simple_plot
    import matplotlib.pyplot as plt
    
    # Single plot with all variables
    fig1 = plot(data, ylabel='Values', title='Combined Parameters')
    
    # Separate subplots for each variable
    fig2 = multi_plot(data, 
                      ylabels=['Temperature (°C)', 'DO (mg/L)'],
                      title='Water Quality Time Series')
    
    # Single variable plot
    fig3 = simple_plot(data['temperature'], 
                       ylabel='Temperature (°C)',
                       title='Temperature Over Time')

Custom Styling
~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.visualization import plot, k2, rainbow, everest
    
    # Use built-in color palettes
    fig1 = plot(data, colors=k2, ylabel='K2 Palette')
    fig2 = plot(data, colors=rainbow, ylabel='Rainbow Palette') 
    fig3 = plot(data, colors=everest, ylabel='Everest Palette')
    
    # Custom styling
    fig4 = plot(data,
                colors=['crimson', 'steelblue', 'forestgreen'],
                style='--',
                linewidth=2,
                figsize=(15, 10),
                ylabel='Custom Styled Plot')

Interactive Plots
~~~~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.visualization import hv_plot
    import holoviews as hv
    
    # Create interactive HoloViews plots
    curves, tooltips = hv_plot(data, width=1200, height=400)
    
    # Display individual parameter
    temperature_plot = curves['temperature']
    
    # Combine multiple parameters
    overlay = hv.Overlay([curves[col] for col in data.columns])
    overlay.opts(legend_position='top_left', title='Interactive Water Quality Data')

Analysis Examples
-----------------

Statistical Analysis
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    import pandas as pd
    import numpy as np
    
    # Basic descriptive statistics
    print("=== Descriptive Statistics ===")
    print(data.describe())
    
    # Monthly aggregations
    monthly_stats = data.resample('M').agg({
        'temperature': ['mean', 'min', 'max', 'std'],
        'do': ['mean', 'median', 'std']
    })
    
    print("\n=== Monthly Statistics ===")
    print(monthly_stats)
    
    # Correlation analysis
    correlation_matrix = data.corr()
    print("\n=== Correlation Matrix ===")
    print(correlation_matrix.round(3))

Data Quality Assessment
~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    def assess_data_quality(df):
        """Comprehensive data quality assessment"""
        report = {}
        
        # Missing values
        missing = df.isnull().sum()
        report['missing_values'] = missing[missing > 0].to_dict()
        
        # Basic statistics
        report['statistics'] = df.describe()
        
        # Potential outliers (using IQR method)
        outliers = {}
        for col in df.select_dtypes(include=[np.number]).columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outlier_count = ((df[col] < (Q1 - 1.5 * IQR)) | 
                           (df[col] > (Q3 + 1.5 * IQR))).sum()
            if outlier_count > 0:
                outliers[col] = outlier_count
        report['potential_outliers'] = outliers
        
        # Data range validation for water quality parameters
        validation = {}
        if 'temperature' in df.columns:
            invalid_temp = ((df['temperature'] < -10) | (df['temperature'] > 50)).sum()
            validation['invalid_temperatures'] = invalid_temp
            
        if 'do' in df.columns:
            invalid_do = ((df['do'] < 0) | (df['do'] > 20)).sum()
            validation['invalid_do'] = invalid_do
            
        report['validation_issues'] = validation
        
        return report
    
    # Assess data quality
    quality_report = assess_data_quality(data)
    
    print("=== Data Quality Report ===")
    for category, results in quality_report.items():
        print(f"\n{category.replace('_', ' ').title()}:")
        if isinstance(results, dict) and results:
            for key, value in results.items():
                print(f"  {key}: {value}")
        elif not results:
            print("  No issues found")

Advanced Visualization
~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # Create comprehensive analysis plot
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # Time series plot
    data.plot(ax=axes[0, 0], title='Time Series')
    axes[0, 0].set_ylabel('Values')
    axes[0, 0].grid(True)
    
    # Correlation heatmap
    sns.heatmap(data.corr(), annot=True, cmap='RdBu_r', center=0, ax=axes[0, 1])
    axes[0, 1].set_title('Parameter Correlations')
    
    # Distribution plots
    data.hist(ax=axes[1, 0], bins=30, alpha=0.7)
    axes[1, 0].set_title('Parameter Distributions')
    
    # Monthly boxplot
    data_monthly = data.copy()
    data_monthly['month'] = data_monthly.index.month
    data_monthly.boxplot(column=['temperature', 'do'], by='month', ax=axes[1, 1])
    axes[1, 1].set_title('Monthly Variations')
    
    plt.tight_layout()
    plt.savefig('comprehensive_analysis.png', dpi=300, bbox_inches='tight')

Batch Processing Examples
-------------------------

Multiple File Processing
~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    import glob
    import os
    from concurrent.futures import ProcessPoolExecutor
    import multiprocessing as mp
    
    def process_single_file(filepath):
        """Process a single model output file"""
        try:
            # Extract year from filename (assuming format: output_2023.csv)
            filename = os.path.basename(filepath)
            year = int(filename.split('_')[1].split('.')[0])
            
            # Read data
            data = read(filepath, year=year, data_columns=['temp', 'do'])
            
            # Calculate monthly averages
            monthly_avg = data.resample('M').mean()
            
            # Create plot
            fig = plot(data, title=f'Results for {year}')
            fig.savefig(f'plot_{year}.png')
            plt.close(fig)
            
            return {
                'file': filepath,
                'year': year,
                'records': len(data),
                'monthly_avg': monthly_avg,
                'success': True
            }
            
        except Exception as e:
            return {
                'file': filepath,
                'error': str(e),
                'success': False
            }
    
    # Find all model output files
    file_pattern = 'model_output_*.csv'
    file_list = glob.glob(file_pattern)
    
    print(f"Found {len(file_list)} files to process")
    
    # Process files in parallel
    with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
        results = list(executor.map(process_single_file, file_list))
    
    # Summarize results
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    print(f"Successfully processed: {len(successful)} files")
    print(f"Failed to process: {len(failed)} files")
    
    if failed:
        print("\nErrors encountered:")
        for result in failed:
            print(f"  {result['file']}: {result['error']}")

Automated Report Generation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.analysis import generate_plots_report
    from cequalw2.fileio import write_plot_control
    import pandas as pd
    
    # Create plot control configuration
    plot_config = pd.DataFrame({
        'Filename': [
            'temperature.csv',
            'dissolved_oxygen.csv', 
            'flow_data.csv'
        ],
        'Columns': [
            ['temperature'],
            ['dissolved_oxygen'],
            ['flow_rate', 'velocity']
        ],
        'Labels': [
            ['Temperature (°C)'],
            ['Dissolved Oxygen (mg/L)'],
            ['Flow Rate (m³/s)', 'Velocity (m/s)']
        ],
        'PlotType': [
            'combined',
            'combined', 
            'subplots'
        ]
    })
    
    # Save configuration for reuse
    write_plot_control(plot_config, 'analysis_config.yaml')
    
    # Generate comprehensive report
    generate_plots_report(
        plot_config,
        model_path='/path/to/model/output',
        outfile='model_analysis_report.md',
        title='CE-QUAL-W2 Model Analysis Report',
        subtitle='Lake Example - Summer 2023 Simulation',
        file_type='png',
        pdf_report=True  # Requires pandoc installation
    )
    
    print("Report generated: model_analysis_report.md")
    if os.path.exists('model_analysis_report.pdf'):
        print("PDF report generated: model_analysis_report.pdf")

Integration Examples
--------------------

With Scientific Libraries
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    # Integration with SciPy for advanced analysis
    from scipy import stats
    from scipy.signal import savgol_filter
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA
    
    # Smooth noisy data
    data['temp_smooth'] = savgol_filter(data['temperature'], 
                                        window_length=11, 
                                        polyorder=3)
    
    # Statistical tests
    statistic, p_value = stats.normaltest(data['temperature'])
    print(f"Normality test p-value: {p_value:.4f}")
    
    # Principal Component Analysis
    features = data[['temperature', 'do']].dropna()
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(features_scaled)
    
    print(f"Explained variance ratio: {pca.explained_variance_ratio_}")

With Geographic Data
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    # Add spatial information (example coordinates)
    data['latitude'] = 45.0 + np.random.normal(0, 0.01, len(data))
    data['longitude'] = -120.0 + np.random.normal(0, 0.01, len(data))
    
    # Create spatial plots (requires geopandas)
    try:
        import geopandas as gpd
        from shapely.geometry import Point
        
        # Create GeoDataFrame
        geometry = [Point(xy) for xy in zip(data.longitude, data.latitude)]
        gdf = gpd.GeoDataFrame(data, geometry=geometry)
        
        # Plot spatial distribution
        fig, ax = plt.subplots(figsize=(10, 8))
        gdf.plot(column='temperature', cmap='RdYlBu_r', ax=ax, legend=True)
        ax.set_title('Spatial Temperature Distribution')
        plt.savefig('spatial_temperature.png')
        
    except ImportError:
        print("GeoPandas not available for spatial analysis")

Database Integration
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    from cequalw2.analysis import sql_query
    import sqlite3
    
    # Create database and store processed data
    conn = sqlite3.connect('analysis_results.db')
    
    # Store original data
    data.to_sql('raw_data', conn, if_exists='replace', index=True)
    
    # Store monthly statistics
    monthly_stats.to_sql('monthly_stats', conn, if_exists='replace', index=True)
    
    # Perform complex queries
    query = """
    SELECT strftime('%Y-%m', index) as month,
           AVG(temperature) as avg_temp,
           AVG(do) as avg_do,
           COUNT(*) as records
    FROM raw_data 
    WHERE temperature > 15.0 
    GROUP BY strftime('%Y-%m', index)
    ORDER BY month
    """
    
    result = sql_query('analysis_results.db', query)
    print("Monthly averages for temperatures > 15°C:")
    print(result)
    
    conn.close()

Performance Examples
--------------------

Memory-Efficient Processing
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    def process_large_file(filepath, chunksize=10000):
        """Process large files in chunks to manage memory"""
        chunks = []
        
        # Read file in chunks
        for chunk in pd.read_csv(filepath, chunksize=chunksize):
            # Process each chunk
            processed_chunk = process_chunk(chunk)
            chunks.append(processed_chunk)
        
        # Combine results
        return pd.concat(chunks, ignore_index=True)
    
    def process_chunk(chunk):
        """Process a single chunk of data"""
        # Convert day-of-year to datetime for this chunk
        if 'day_of_year' in chunk.columns:
            dates = day_of_year_to_date(2023, chunk['day_of_year'].tolist())
            chunk.index = dates
            chunk.drop('day_of_year', axis=1, inplace=True)
        
        # Calculate rolling averages to reduce data size
        chunk_processed = chunk.rolling(window=24, min_periods=1).mean()
        
        return chunk_processed

Optimization Tips
~~~~~~~~~~~~~~~~~

.. code-block:: python

    import time
    
    # Time different approaches
    start_time = time.time()
    
    # Method 1: Read all columns then select
    data_all = read('large_file.csv', year=2023)
    data_subset = data_all[['temperature', 'do']]
    
    method1_time = time.time() - start_time
    
    # Method 2: Read only required columns
    start_time = time.time()
    data_direct = read('large_file.csv', year=2023, data_columns=['temperature', 'do'])
    method2_time = time.time() - start_time
    
    print(f"Method 1 (read all, then select): {method1_time:.2f} seconds")
    print(f"Method 2 (read specific columns): {method2_time:.2f} seconds")
    print(f"Speedup: {method1_time/method2_time:.1f}x")

Complete Workflow Example
-------------------------

End-to-End Analysis
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    """
    Complete workflow example: From raw data to final report
    """
    
    import os
    import pandas as pd
    from cequalw2.fileio import read
    from cequalw2.visualization import plot, multi_plot
    from cequalw2.analysis import generate_plots_report, write_csv
    
    def complete_analysis_workflow(input_file, output_dir, year=2023):
        """Complete analysis workflow"""
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        print("1. Reading data...")
        data = read(input_file, year=year, data_columns=['temp', 'do', 'ph'])
        print(f"   Loaded {len(data)} records from {data.index.min()} to {data.index.max()}")
        
        print("2. Data quality assessment...")
        quality_report = assess_data_quality(data)
        
        print("3. Statistical analysis...")
        stats = data.describe()
        monthly_stats = data.resample('M').mean()
        
        print("4. Creating visualizations...")
        # Time series plot
        fig1 = plot(data, ylabel='Water Quality Parameters', 
                   title='Complete Time Series')
        fig1.savefig(f'{output_dir}/time_series.png', dpi=300, bbox_inches='tight')
        
        # Monthly subplots
        fig2 = multi_plot(monthly_stats, 
                         ylabels=['Temperature (°C)', 'DO (mg/L)', 'pH'],
                         title='Monthly Averages')
        fig2.savefig(f'{output_dir}/monthly_trends.png', dpi=300, bbox_inches='tight')
        
        print("5. Exporting processed data...")
        # Export monthly averages
        write_csv(monthly_stats, f'{output_dir}/monthly_averages.csv', year=year)
        
        # Export to Excel with multiple sheets
        with pd.ExcelWriter(f'{output_dir}/analysis_results.xlsx') as writer:
            data.to_excel(writer, sheet_name='Raw Data')
            stats.to_excel(writer, sheet_name='Statistics')
            monthly_stats.to_excel(writer, sheet_name='Monthly Averages')
        
        print("6. Generating report...")
        report_content = f"""
    # Water Quality Analysis Report
    
    ## Summary
    - **Data Period**: {data.index.min().strftime('%Y-%m-%d')} to {data.index.max().strftime('%Y-%m-%d')}
    - **Total Records**: {len(data):,}
    - **Parameters**: {', '.join(data.columns)}
    
    ## Statistics
    {stats.round(3).to_markdown()}
    
    ## Monthly Trends
    {monthly_stats.round(3).to_markdown()}
    
    ## Visualizations
    ![Time Series](time_series.png)
    ![Monthly Trends](monthly_trends.png)
    
    ## Data Quality
    - Missing values: {quality_report.get('missing_values', 'None')}
    - Potential outliers: {quality_report.get('potential_outliers', 'None')}
    - Validation issues: {quality_report.get('validation_issues', 'None')}
    """
        
        with open(f'{output_dir}/analysis_report.md', 'w') as f:
            f.write(report_content)
        
        print(f"Analysis complete! Results saved to: {output_dir}")
        return data, stats, monthly_stats
    
    # Run complete workflow
    if __name__ == "__main__":
        input_file = "model_output.csv"
        output_directory = "analysis_results"
        
        results = complete_analysis_workflow(input_file, output_directory, year=2023)
        
        print("\\nWorkflow completed successfully!")

This complete example demonstrates how to integrate all the library's capabilities into a comprehensive analysis workflow that can be easily adapted for different CE-QUAL-W2 modeling projects.